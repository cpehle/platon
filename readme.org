* Introduction

Platon aims to be a general purpose programming language that is
extensible and supports unconventional programming models. 
The implementation will not be a monolithic compiler, but is
structured around a meta language for program transformation and
symbolic computation.

One of the main distiguishing features is that platon is designed to
support typed symbolic programming over an arbitrary base monoidal
category.

The intention is for this language to be based on ideas from
mathematics and features determined from case studies. By including
only features that have a sound mathematical theory behind them, we
hope to design a language that does not contain unnescessary
complexity, by considering use cases we hope to ground our design in
reality. We believe that given the choice a language should always try
subsume as many features as possible by a general mechanism. An
example is error handling: Clearly any large scale program needs some
form of error handling, 


* Theory

There is a correspondence between simply typed lambda calculus and the
internal logical of cartesian closed categories. In simply typed
lambda calculus.

In simply typed lambda calculus, given a set of base types $base$ the
types of the theory are

$type t = t \to t | base | t \times t$ 

Here $\to$ denotes a left associative binary operation and $\times$
the cartesian product.

The terms of the calculus are:

$type t = variable | \lambda variable * Type.t * t | App t * t | \cross t * t | \pi_k | constant$





Given a monoidal catgory (M, \otimes, I) one can formulate a number
of algebraic structures internal to the category M. A lot of them
arise as functors from certain small categories into M.

Natural transformations have the interpretation of communication
patterns and optimizations.

Zero cost abstractions (See for example HardCaml for how this is done
by metaprogramming, aswell as Common Lisp / C++ / Rust).


* Implementation

** Propagators

** Linear types

** Symbolic programming

- PDE
- Useful for things like Deep Learning

** Optimization
- read paper on Stratego
- optimization via saturation (machine learning applied to optimization)
- need well defined notion of program equivalence and then introduce stochastic walk in space of possible equivalent programs
- whole program optimization and transformation
- see corelets in fftw
- pipelines (optimization of pipelines by partial evaluation)
- partial evaluation
- shift between compile time and evaluation time
- hotspot JIT
- dynamic data / code locallity improvement (i.e. learn access patterns and optimize for them)

** Features
Monoidal categories (Particle analogy), linear logic, syntactic
control of interference. (Patterns of information)

Take inspiration from Magma and Mathematica

Generalized algebraic data types
Higher kinded types
Extensible records (take inspiration from Cedar/Oberon/Purescript)
Kind polymorphism
Row polymorphism
Algebraic effects?


*** Concurrency 
- One shot continuations (require linear type system)

*** Type system

- Should support linear types to model resources.

  
*** Computational model

**** Agents fibered over manifold connected by "signal net"

**** Geometry of Computation

"Slogan": Ordinary computation == Computation over a point

Instead consider a sheaf of monoidal categories over a space
Investigate the following computational model:

- Primitives are nodes (cells) and connections between them, they 
  communicate via asynchronous message passing.
- Each node has no knowledge of which nodes connect to it, that is it
  can receive messages from anyone that knows its "address".  Every node
  has knowledge to which nodes it is connected to and those can change
  over time.
- Generalize to N dimensions, that is "Edges", "Faces" etc. can have
  state that is computed from lower and higher order information, faces
  and so on can split and merge, etc.
- Computational primitives are: Have maps \delta^i_k (k = 0..i) that
  map the (i+1) dimensional simplex to one of its faces \colon
  \Delta^{i+1} \to \Delta^{i}. Similarly have i+1 maps that map the
  i-simplex to the faces of the i+1 simplex. Given an n-simplex
  consider a map that assigns to every vertex a group G_i, to every
  edge an group homomorphism $g_{ij} \colon G_i \to G_j$.



Take inspiration from Biology:

- Message passing / signaling, very complex activation and reaction chains,
  separation of compuation "domains", that is cells. Nesting of cells within cells.
  Asynchronous continuous time behaviour.
- Investigate different discretetization algorithms
- Take it as an inspiration for system design: Separation of concern
  fault tolerance etc.
- Use it as an inspiration for describing computations that take
  "energy" and to dynamically schedule different processes, as
  inspiration for generalized state machines


** Use Cases

Model language requirements after use cases. Generally speaking I
think all use cases can only be met if it is easy to extend the
language and write domain specific languages in it (see Haskell,
Fortress, work by VPRI as examples)

*** Systems Programming

Support for state machines / protocols as seen in the Singular# language.


*** Computer Algebra



*** Numerical code
Besides what Fortran covers exceptionally well, in principle one need
both support for heterogenous memory architectures (see Languages like
Chapel..), that is "domains of computation", aswell as message
passing. More importantly there should also be good support for High
Performance Computing with non-homogenous data.

*** Graphics

*** Biology (Genomic Data, Molecular Biology, Neural Networks)

*** Finance


## Typesystem

Platon supports a wide variety of different algebraic structures and
efficient implementations of them, similar to Magma.

The system is supposed to be general enough so that for each notion
category the user implements, together with potential monoidal
product the supported algebraic structures work.

(Investigate Twitters Scala library for inspiration)


C : Cat
m : C x C -> m C C
b : m C C \to m C C
m x y = m y x
i : I -> C






A computer system has some finite sized main memory, caches, disks,
diskcaches etc., denote them by M_{i}. Then whenever it is possible to
transfer data from M_{i} to M_{j} denote this by an arrow A_{ij}.

Assume for the moment that each of the memory spaces are linear
addressable, that is they are all characterized by a tuple (V,d),
where V is the base configuration space of the memory and d is the
dimension.

For a map $A_{ij}$ to exist at all one then has to have a map from
$V_{i}^{k}$ to $V_{j}^{l}$ for some $k < d_{i}, l < d_{j}$.

*** Simulation

